{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías requeridas para el funcionamiento del programa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables Globales\n",
    "\n",
    "# Ruta a archivo csv\n",
    "ruta_csv = 'trips.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCION DE DESCARGA Y TRANSFORMACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función para carga de archivo csv y transformación a Dataframe\n",
    "\n",
    "def descarga_data (ruta: str) -> pd.DataFrame:\n",
    "\n",
    "    with open (ruta_csv) as csv:\n",
    "        df = pd.read_csv(csv)\n",
    "        return df\n",
    "\n",
    "#Función para transformar data\n",
    "def transformar_data (df: pd.DataFrame) -> pd.DataFrame:\n",
    "    #Transformaciones para obtener latitud y longitud del punto/coordenada de origen\n",
    "    # Se utiliza el método split para obtener coordenadas en columnas distintas y \n",
    "    # así poder agrupar por la parte entera o flotante de la coordenada, asumiendo que \n",
    "    # similares coordenas implica puntos de origen/destino similares\n",
    "    origen = df.origin_coord.str.split(expand = True)\n",
    "    origen = origen.rename(columns = {0:\"point\", 1:\"lat_o\", 2:\"lon_o\"})\n",
    "    lat = origen.lat_o.str.split('(',expand=True)\n",
    "    lat = lat.rename(columns = {0:\"(\", 1:\"lat_origin\"})\n",
    "    lat = lat[[\"lat_origin\"]]\n",
    "    lon = origen.lon_o.str.split(')', expand=True)\n",
    "    lon = lon.rename(columns = {0:\"lon_origin\", 1:\")\"})\n",
    "    lon = lon[[\"lon_origin\"]]\n",
    "    origin_coord = pd.concat([lat, lon], axis=1)\n",
    "    df_v1 = pd.concat([df, origin_coord], axis=1)\n",
    "    #Transformaciones para obtener latitud y longitud del punto/coordenada de destino\n",
    "    destino = df.destination_coord.str.split(expand = True)\n",
    "    destino = destino.rename(columns = {0:\"point\", 1:\"lat_d\", 2:\"lon_d\"})\n",
    "    lat_d = destino.lat_d.str.split('(',expand=True)\n",
    "    lat_d = lat_d.rename(columns = {0:\"(\", 1:\"lat_destination\"})\n",
    "    lat_d = lat_d[[\"lat_destination\"]]\n",
    "    lon_d = destino.lon_d.str.split(')', expand=True)\n",
    "    lon_d = lon_d.rename(columns = {0:\"lon_destination\", 1:\")\"})\n",
    "    lon_d = lon_d[[\"lon_destination\"]]\n",
    "    destination_coord = pd.concat([lat_d, lon_d], axis=1)\n",
    "    df_v2 = pd.concat([df_v1, destination_coord], axis=1)\n",
    "    \n",
    "    fechahora = df.datetime\n",
    "    fechahora = pd.to_datetime(fechahora, errors=\"coerce\")\n",
    "    fechahora = fechahora.rename(index=\"fechahora\")\n",
    "    df_v3 = pd.concat([df_v2, fechahora], axis=1)\n",
    "    df_v3[\"hora\"] = df_v3.fechahora.dt.hour\n",
    "    #Me di cuenta que no podía simplemente hacer un split de la cadena, \n",
    "    # sino que debía manipular el string como flotante para obtener la parte entera \n",
    "    # y la parte decimal de cada coordenada (latitud y longitud) \n",
    "    #Se obtiene parte entera  y decimal de latitud de origen en columnas\n",
    "    df_v3[\"lat_o_int\"] = df_v3[\"lat_origin\"].astype(float)\n",
    "    df_v3[\"lat_o_int\"] = df_v3[\"lat_o_int\"].astype(int)\n",
    "    df_v3[\"lat_o_float\"] = df_v3[\"lat_origin\"].astype(str).str.split('.').str[1]\n",
    "    df_v3[\"lat_o_float\"] = (df_v3[\"lat_o_float\"]).str.slice(0, 1)\n",
    "\n",
    "    #Se obtiene parte entera  y decimal de longitud de origen en columnas\n",
    "    df_v3[\"lon_o_int\"] = df_v3[\"lon_origin\"].astype(float)\n",
    "    df_v3[\"lon_o_int\"] = df_v3[\"lon_o_int\"].astype(int)\n",
    "    df_v3[\"lon_o_float\"] = df_v3[\"lon_origin\"].astype(str).str.split('.').str[1]\n",
    "    df_v3[\"lon_o_float\"] = (df_v3[\"lon_o_float\"]).str.slice(0, 1)\n",
    "\n",
    "    #Se obtiene parte entera  y decimal de latitud de destino en columnas\n",
    "    df_v3[\"lat_d_int\"] = df_v3[\"lat_destination\"].astype(float)\n",
    "    df_v3[\"lat_d_int\"] = df_v3[\"lat_d_int\"].astype(int)\n",
    "    df_v3[\"lat_d_float\"] = df_v3[\"lat_destination\"].astype(str).str.split('.').str[1]\n",
    "    df_v3[\"lat_d_float\"] = (df_v3[\"lat_d_float\"]).str.slice(0, 1)\n",
    "\n",
    "    #Se obtiene parte entera  y decimal de longitud de destino en columnas\n",
    "    df_v3[\"lon_d_int\"] = df_v3[\"lon_destination\"].astype(float)\n",
    "    df_v3[\"lon_d_int\"] = df_v3[\"lon_d_int\"].astype(int)\n",
    "    df_v3[\"lon_d_float\"] = df_v3[\"lon_destination\"].astype(str).str.split('.').str[1]\n",
    "    df_v3[\"lon_d_float\"] = (df_v3[\"lon_d_float\"]).str.slice(0, 1)\n",
    "    #Finalmente para decidir la agrupación de datos hice pruebas de agrupación con la parte entera de latitud y longitud de las coordenadas \n",
    "    # y con la parte entera y flotante a la vez, pero al incluir la parte flotante, los casos similares eran sólo 2\n",
    "    #Es de esperar que con una base más amplia tenga sentido considerar la parte flotante de las coordenadas en la agrupación...\n",
    "    \n",
    "    #OPCION 1 RETURN \n",
    "    #Se crea columna \"cantidad_similar_en_region\" para representar mediante un número entero la cantidad de veces que la fila agrupada por hora, longitud y latitud de origen y destino (parte entera), se repite en la base de datos\n",
    "     #De esta forma se pueden filtrar las filas > 1, que serían aquellas que sí representan viajes similares\n",
    "    df_v3[\"cantidad_similar_en_region\"] = df_v3.groupby(['region','hora', 'lat_o_int', 'lon_o_int', 'lat_d_int','lon_d_int'])['region'].transform('count')\n",
    "   \n",
    "    #OPCION 2 RETURN \n",
    "    #para no aumentar el peso de la data podría limpiarse el dataframe y solo mantener las columnas transformadas:\n",
    "    df_v4 = df_v3[['region', 'fechahora', 'hora', 'lat_origin', 'lat_o_int', 'lat_o_float', 'lon_origin', 'lon_o_int', 'lon_o_float', 'lat_destination', 'lat_d_int', 'lat_d_float', 'lon_destination', 'lon_d_int', 'lon_d_float', 'cantidad_similar_en_region']]\n",
    "    \n",
    "    #OPCION 3 \n",
    "    #Otro enfoque sería simplemente a partir de las columnas creadas: hora, lat_o_int, lon_o_int, lat_d_int y lon_d_int, se aplique una función de agregación en el gestor de base de datos o datawarehouse donde se almacenará la totalidad de la data y no solo este archivo csv.\n",
    "    \n",
    "    return df_v3, df_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMENTARIOS EXTRAS A LA AGRUPACIÓN     \n",
    "\n",
    "#Una forma de agrupar los datos era crear distintos dataframes según se cumplían las condiciones de coordenadas de origen\n",
    "#y destino, y hora similares pero es impracticable ya que los valores de las variables constituyen un rango muy amplio\n",
    "\n",
    "# df_0a4 = df_v3[(df_v3[\"hora\"] < 5) & (df_v3[\"hora\"] >= 0)]\n",
    "# df_5a9 = df_v3[(df_v3[\"hora\"] < 10) & (df_v3[\"hora\"] >= 5)]\n",
    "# df_10a14 = df_v3[(df_v3[\"hora\"] < 15) & (df_v3[\"hora\"] >= 10)]\n",
    "# df_15a19 = df_v3[(df_v3[\"hora\"] < 20) & (df_v3[\"hora\"] >= 15)]\n",
    "# df_20a24 = df_v3[(df_v3[\"hora\"] < 25) & (df_v3[\"hora\"] >= 20)]\n",
    "# df_0a4[\"rango_horario\"] = '0 a 4'\n",
    "# df_5a9[\"rango_horario\"] = '5 a 9'\n",
    "# df_10a14[\"rango_horario\"] = '10 a 14'\n",
    "# df_15a19[\"rango_horario\"] = '15 a 19'\n",
    "# df_20a24[\"rango_horario\"] = '20 a 24'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCIÓN DE CARGA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pg8000\n",
    "import sqlalchemy\n",
    "from google.cloud.sql.connector import Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función auxiliar para establecer la conexión a la base de datos PostgreSql con conector Google CLoud SQL\n",
    "def create_connection():\n",
    "    POSTGRES_CONNECTION_NAME = \"project_name:zone:dbsql_name\"\n",
    "    POSTGRES_USER = \"postgres\"\n",
    "    POSTGRES_PASS = \"postgres\"\n",
    "    POSTGRES_DB = \"dbsql_name\"\n",
    "\n",
    "    connector = Connector()\n",
    "\n",
    "    def init_connection_engine() -> sqlalchemy.engine.Engine:\n",
    "        def getconn() -> pg8000.dbapi.Connection:\n",
    "            conn: pg8000.dbapi.Connection = connector.connect(\n",
    "                POSTGRES_CONNECTION_NAME,\n",
    "                \"pg8000\",\n",
    "                user=POSTGRES_USER,\n",
    "                password=POSTGRES_PASS,\n",
    "                db=POSTGRES_DB,\n",
    "            )\n",
    "            return conn\n",
    "\n",
    "        engine = sqlalchemy.create_engine(\n",
    "            \"postgresql+pg8000://\",\n",
    "            creator=getconn,\n",
    "        )\n",
    "        engine.dialect.description_encoding = None\n",
    "        return engine\n",
    "\n",
    "    db = init_connection_engine()\n",
    "\n",
    "    return db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_data(df : pd.DataFrame):\n",
    "    schema = \"data_geografica\"\n",
    "    table = \"viajes\"\n",
    "\n",
    "    connection = create_connection()\n",
    "\n",
    "    connection.execute('''TRUNCATE TABLE {0}.{1}'''.format(schema, table))\n",
    "    df.to_sql(table, connection, schema=schema, if_exists='append', method='multi', index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba funciones de descarga y transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se ejecuta función descarga_data()\n",
    "df = descarga_data(ruta_csv)\n",
    "#Se ejecuta función transformar_data()\n",
    "dfinal_1, dfinal_2 = transformar_data(df) # Se obtienen dos df según las opciones que se presentaron anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPCIÓN 1 RETURN DE LA FUNCIÓN DESCARGA_DATA()\n",
    "\n",
    "dfinal_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPCIÓN 2 RETURN DE LA FUNCIÓN DESCARGA_DATA()\n",
    "\n",
    "dfinal_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importación de librerías\n",
    "from shapely.geometry import Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A partir de la función descarga_data, se crea un nuevo df_coord con las columnas que se utilizarán para obtener el promedio por bounding box y región"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\AppData\\Local\\Temp\\ipykernel_15168\\3069215760.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_coord_2.rename(columns={\"lat_destination\":\"lat_origin\", \"lon_destination\": \"lon_origin\"},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Se unifican coordenadas de longitud y latitud independiente de origen y destino para utlizar coordenadas en creación de bounding box\n",
    "df_coord = dfinal_1[[\"region\",\"lat_origin\", \"lon_origin\"]]\n",
    "df_coord_2= dfinal_1[[\"region\",\"lat_destination\", \"lon_destination\"]]\n",
    "df_coord_2.rename(columns={\"lat_destination\":\"lat_origin\", \"lon_destination\": \"lon_origin\"},inplace=True)\n",
    "df_coord = pd.concat([df_coord, df_coord_2],)\n",
    "df_coord.rename(columns={\"lat_origin\":\"latitud\", \"lon_origin\": \"longitud\"},inplace=True)\n",
    "df_coord = df_coord.astype({\"latitud\" :'float', \"longitud\":'float'})\n",
    "df_coord.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Función crea bounding box, la cual llamaremos en la funcion promedios_viajes() y le pasaremos los valores min y max de latitud y longitud por región\n",
    "def bbox(long0, lat0, lat1, long1):\n",
    "        return Polygon([[long0, lat0],\n",
    "                        [long1,lat0],\n",
    "                        [long1,lat1],\n",
    "                        [long0, lat1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def promedios_viajes (df_coord):\n",
    "    regiones = df_coord.region.unique()\n",
    "    for r in regiones:\n",
    "        contador = 0\n",
    "        df_coor_region = df_coord[df_coord[\"region\"]==r]\n",
    "        max_lat = df_coord.latitud.max()\n",
    "        min_lat = df_coord.latitud.min()\n",
    "        max_lon = df_coord.longitud.min()\n",
    "        min_lon = df_coord.longitud.max()\n",
    "        bounding_box= bbox(max_lon, max_lon, min_lat, min_lon)\n",
    "        total_filas = len(df_coor_region)\n",
    "        for index, row in df_coor_region.iterrows():\n",
    "            if min_lat < row[\"latitud\"] < max_lat:\n",
    "                contador +=1\n",
    "            else: \n",
    "                pass\n",
    "        promedio = contador/total_filas\n",
    "        print(r + ' promedio = ' + str(promedio) + ' en bounding box ' + str(bounding_box))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba función promedios_viajes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prague promedio = 0.9852941176470589 en bounding box POLYGON ((44.97612466562052 44.97612466562052, 53.65419620043016 44.97612466562052, 53.65419620043016 7.513135087952872, 44.97612466562052 7.513135087952872, 44.97612466562052 44.97612466562052))\n",
      "Turin promedio = 0.9868421052631579 en bounding box POLYGON ((44.97612466562052 44.97612466562052, 53.65419620043016 44.97612466562052, 53.65419620043016 7.513135087952872, 44.97612466562052 7.513135087952872, 44.97612466562052 44.97612466562052))\n",
      "Hamburg promedio = 1.0 en bounding box POLYGON ((44.97612466562052 44.97612466562052, 53.65419620043016 44.97612466562052, 53.65419620043016 7.513135087952872, 44.97612466562052 7.513135087952872, 44.97612466562052 44.97612466562052))\n"
     ]
    }
   ],
   "source": [
    "promedios_viajes(df_coord)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c812eb984687e98b7686c463c3fae2045e58b2a2b4e065b3dcebe3cb7d51662e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
